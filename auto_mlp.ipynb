{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "import keras\n",
    "from keras import optimizers \n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding \n",
    "from keras .models import Sequential \n",
    "from keras.preprocessing.text import Tokenizer          \n",
    "from keras.preprocessing.sequence import pad_sequences  \n",
    "from keras.utils import to_categorical                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv(\"train.csv\",header=0)\n",
    "test = pd.read_csv(\"test.csv\",header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['QUERIES FROM PHARMACY', 'NEW APPOINTMENT', 'OTHERS',\n",
       "       'MEDICATION RELATED',\n",
       "       'SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)', 'REFILL',\n",
       "       'PRIOR AUTHORIZATION', 'RESCHEDULING', 'SYMPTOMS', 'LAB RESULTS',\n",
       "       'FOLLOW UP ON PREVIOUS REQUEST', 'PROVIDER', 'CHANGE OF PROVIDER',\n",
       "       'SHARING OF LAB RECORDS (FAX, E-MAIL, ETC.)',\n",
       "       'QUERY ON CURRENT APPOINTMENT', 'RUNNING LATE TO APPOINTMENT',\n",
       "       'CANCELLATION', 'CHANGE OF PHARMACY', 'QUERIES FROM INSURANCE FIRM',\n",
       "       'JUNK', 'CHANGE OF HOSPITAL'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.categories.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48667, 2)\n"
     ]
    }
   ],
   "source": [
    "train=train.dropna(axis=0)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEDICATION RELATED                               9010\n",
       "NEW APPOINTMENT                                  8907\n",
       "REFILL                                           8347\n",
       "OTHERS                                           6232\n",
       "SHARING OF HEALTH RECORDS (FAX, E-MAIL, ETC.)    3018\n",
       "LAB RESULTS                                      2253\n",
       "PROVIDER                                         1677\n",
       "QUERIES FROM PHARMACY                            1464\n",
       "RESCHEDULING                                     1382\n",
       "SHARING OF LAB RECORDS (FAX, E-MAIL, ETC.)       1212\n",
       "PRIOR AUTHORIZATION                              1043\n",
       "SYMPTOMS                                         1021\n",
       "CHANGE OF PROVIDER                                811\n",
       "RUNNING LATE TO APPOINTMENT                       590\n",
       "CANCELLATION                                      563\n",
       "QUERY ON CURRENT APPOINTMENT                      559\n",
       "FOLLOW UP ON PREVIOUS REQUEST                     304\n",
       "CHANGE OF HOSPITAL                                127\n",
       "QUERIES FROM INSURANCE FIRM                        91\n",
       "CHANGE OF PHARMACY                                 47\n",
       "JUNK                                                9\n",
       "Name: categories, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.categories.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.converse=train.converse.astype('str')\n",
    "test.converse=test.converse.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.categories=train.categories.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 463# Sequence length of each sentence. If more, crop. If less, pad with zeros\n",
    "MAX_NB_WORDS = 20000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35747 unique tokens.\n",
      "(48667, 463)\n",
      "(8581, 463)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)   # get the frequently occuring words\n",
    "tokenizer.fit_on_texts(train.converse)           \n",
    "train_sequences = tokenizer.texts_to_sequences(train.converse)\n",
    "test_sequences = tokenizer.texts_to_sequences(test.converse)\n",
    "\n",
    "word_index = tokenizer.word_index               # dictionary containing words and their index\n",
    "# print(tokenizer.word_index)                   # print to check\n",
    "print('Found %s unique tokens.' % len(word_index)) # total words in the corpus\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH) # get only the top frequent words on train\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)   # get only the top frequent words on test\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = list(train.categories.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y=np.array([train_label.index(i) for i in train.categories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48667,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48667, 21)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_y.shape)\n",
    "train_y=to_categorical(train_y)\n",
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((48667, 463), (8581, 463))\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(463,)))\n",
    "mlp.add(Dense(30, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(21, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(21, activation='softmax'))\n",
    "\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "48667/48667 [==============================] - 5s - loss: 2.4423 - acc: 0.2160     \n",
      "Epoch 2/10\n",
      "48667/48667 [==============================] - 4s - loss: 2.3057 - acc: 0.2675     \n",
      "Epoch 3/10\n",
      "48667/48667 [==============================] - 4s - loss: 2.2751 - acc: 0.2787     \n",
      "Epoch 4/10\n",
      "48667/48667 [==============================] - 4s - loss: 2.2599 - acc: 0.2806     \n",
      "Epoch 5/10\n",
      "48667/48667 [==============================] - 4s - loss: 2.2548 - acc: 0.2807     \n",
      "Epoch 6/10\n",
      "48667/48667 [==============================] - 4s - loss: 2.2501 - acc: 0.2821     \n",
      "Epoch 7/10\n",
      "48667/48667 [==============================] - 5s - loss: 2.2437 - acc: 0.2824     \n",
      "Epoch 8/10\n",
      "48667/48667 [==============================] - 4s - loss: 2.2409 - acc: 0.2828     \n",
      "Epoch 9/10\n",
      "48667/48667 [==============================] - 5s - loss: 2.2395 - acc: 0.2829     \n",
      "Epoch 10/10\n",
      "48667/48667 [==============================] - 4s - loss: 2.2334 - acc: 0.2846     \n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 10      # number of epochs\n",
    "batch_size = 32    # batch size\n",
    "history = mlp.fit(train_data, train_y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for array operations\n",
    "from keras.models import Model, Sequential # for defining the architectures\n",
    "from keras.layers import Dense, Dropout, Input # layers for building the network\n",
    "from keras.utils import to_categorical # to_categorical does one-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 10      # number of epochs\n",
    "batch_size = 32    # batch size\n",
    "\n",
    "input_dat = Input(shape=(463,))\n",
    "crrpt_dat = Dropout(0.5)(input_dat)\n",
    "encoded = Dense(400, activation='sigmoid')(crrpt_dat)\n",
    "decoded = Dense(21, activation='linear')(encoded)\n",
    "\n",
    "autoencoder = Model(input_dat,decoded)\n",
    "autoencoder.compile(optimizer='adam',\n",
    "                    loss='mean_squared_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48667, 463)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cython\n",
      "  Downloading Cython-0.27.3-cp27-cp27mu-manylinux1_x86_64.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.0MB 234kB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: cython\n",
      "Successfully installed cython-0.27.3\n"
     ]
    }
   ],
   "source": [
    "! pip install --user cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "48667/48667 [==============================] - 8s - loss: 0.0395     \n",
      "Epoch 2/10\n",
      "48667/48667 [==============================] - 8s - loss: 0.0396     \n",
      "Epoch 3/10\n",
      "48667/48667 [==============================] - 8s - loss: 0.0396     \n",
      "Epoch 4/10\n",
      "48667/48667 [==============================] - 8s - loss: 0.0395     \n",
      "Epoch 5/10\n",
      "48667/48667 [==============================] - 9s - loss: 0.0396     \n",
      "Epoch 6/10\n",
      "48667/48667 [==============================] - 8s - loss: 0.0395     \n",
      "Epoch 7/10\n",
      "48667/48667 [==============================] - 8s - loss: 0.0395     \n",
      "Epoch 8/10\n",
      "48667/48667 [==============================] - 8s - loss: 0.0395     \n",
      "Epoch 9/10\n",
      "48667/48667 [==============================] - 8s - loss: 0.0395     \n",
      "Epoch 10/10\n",
      "48667/48667 [==============================] - 8s - loss: 0.0395     \n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(train_data, train_y,  \n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    )\n",
    "\n",
    "import h5py\n",
    "#autoencoder.save_weights('data_model.h5py', 'r') # save the model weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_dat,encoded)\n",
    "htrain_data = encoder.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 3s - loss: 2.3026 - acc: 0.2879     \n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 4s - loss: 2.1906 - acc: 0.3196     \n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s - loss: 2.1656 - acc: 0.3233     \n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 4s - loss: 2.1556 - acc: 0.3278     \n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 3s - loss: 2.1445 - acc: 0.3285     \n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 3s - loss: 2.1404 - acc: 0.3305     \n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 3s - loss: 2.1307 - acc: 0.3346     \n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 3s - loss: 2.1309 - acc: 0.3326     \n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 3s - loss: 2.1225 - acc: 0.3378     \n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 4s - loss: 2.1185 - acc: 0.3366     \n"
     ]
    }
   ],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Dropout(0.2, input_shape=(400,)))\n",
    "mlp.add(Dense(400, activation='sigmoid'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(21, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "htest_data = encoder.predict(test_data)\n",
    "history = mlp.fit(htrain_data[:20000], train_y[:20000],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
